{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User identification based on keystroke dynamics\n",
    "## PL in ML 2017\n",
    "### Maciej Dziubiński @ Nethone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "1. [The dataset](#dataset)\n",
    "2. [Feature extraction](#features)\n",
    "3. [Training and test sets](#splitting)\n",
    "4. [Custom cross-validation](#crossval)\n",
    "5. [Model I: XGBoost](#xgboost)\n",
    "6. [An interlude (AUC, feature scaling)](#interlude)\n",
    "7. [Model II: the multilayer perceptron](#mlp)\n",
    "8. [Model III: simple siamese model](#siamese)\n",
    "9. [Model IV: RNN siamese model](#siamese_rnn)\n",
    "10. [Tests on verification data](#testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.style.use('seaborn-dark-palette')\n",
    "mpl.style.use('seaborn-whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "# The xgboost package throws a DeprecationWarning which we'll ignore\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in the data (\"DataSet A\" only) <a name=\"dataset\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the [Bei Hang dataset](http://mpl.buaa.edu.cn/detail1.htm) which contains keystroke dynamics of the form:\n",
    "\n",
    "[press1, release1, press2, release2, ...]\n",
    "\n",
    "or:\n",
    "\n",
    "**[P1, R1, P2, R2, ...]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/keystroke_only.png\" alt=\"Drawing\" style=\"width: 500;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two datasets: \"Dataset A\", and \"Dataset B\", but we’ll only use the first one (collected in a cybercafe environment).\n",
    "\n",
    "There are 49 users in \"Dataset A\", each with several keystroke timestamp sequences: [P1, R1, P2, R2, ...] produced while typing in a password during **registration** (depicted in violet below).\n",
    "Each user has his own password, and in general different users have passwords of different lengths.\n",
    "\n",
    "Additionally, a set of testing sequences was collected (**verification data** depicted in blue below), which we'll use for testing our model.\n",
    "\n",
    "<img src=\"images/one_user.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from read_data import produce_whole_DF\n",
    "\n",
    "\n",
    "DF_whole = produce_whole_DF('BeiHang')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_whole.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plots import plot_user_sequences\n",
    "\n",
    "\n",
    "user_name = 'zhuzhu'\n",
    "plot_user_sequences(DF_whole, user_name)\n",
    "plot_user_sequences(DF_whole, user_name, np.diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_name = 'qianhai18'\n",
    "plot_user_sequences(DF_whole, user_name)\n",
    "plot_user_sequences(DF_whole, user_name, np.diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Crude) feature extraction <a name=\"features\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most commonly used (sub-)sequences extracted from the keystroke sequence are the **dwell** and **flight times**.\n",
    "\n",
    "![](images/keystroke.png)\n",
    "\n",
    "As features, we'll use the **mean** and **standard deviation** of the dwell and flight times.\n",
    "\n",
    "We'll also use the mean and std of the first order difference (`diff[n] = seq[n+1] - seq[n]`), using the `np.diff` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_flight_times(seq):\n",
    "    \"\"\"Flight time -- time between key up and the next key down\"\"\"\n",
    "    return seq[2::2] - seq[1:-1:2]\n",
    "\n",
    "\n",
    "def extract_dwell_times(seq):\n",
    "    \"\"\"Dwell time -- time spent in pressing a key\"\"\"\n",
    "    return seq[1::2] - seq[::2]\n",
    "    \n",
    "    \n",
    "def extract_features(seq):\n",
    "    \"\"\"Function calculating basic statistic from a sequence of\n",
    "    keystroke timestamps (argument `seq`).\n",
    "    \"\"\"\n",
    "    diff = np.diff(seq)\n",
    "    dwell = extract_dwell_times(seq)\n",
    "    flight = extract_flight_times(seq)\n",
    "    # TODO(1): add more features\n",
    "\n",
    "    return {\n",
    "        'diff_mean': diff.mean(),\n",
    "        'diff_std': diff.std(),\n",
    "        'dwell_mean': dwell.mean(),\n",
    "        'dwell_std': dwell.std(),\n",
    "        'flight_mean': flight.mean(),\n",
    "        'flight_std': flight.std(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user_name = 'zhuzhu'\n",
    "plot_user_sequences(DF_whole, user_name, extract_dwell_times)\n",
    "plot_user_sequences(DF_whole, user_name, extract_flight_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user_name = 'qianhai18'\n",
    "plot_user_sequences(DF_whole, user_name, extract_dwell_times)\n",
    "plot_user_sequences(DF_whole, user_name, extract_flight_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DF_features = (DF_whole['sequence']\n",
    "               .apply(extract_features)\n",
    "               .apply(pd.Series))\n",
    "\n",
    "base_feature_names = DF_features.columns.tolist()\n",
    "DF_whole = pd.concat([DF_whole, DF_features], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_whole.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split into train and test <a name=\"splitting\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_user_names = sorted([\n",
    "    'wukun88',\n",
    "    'wwwh',\n",
    "    'x335399102',\n",
    "    'xiaoji',\n",
    "    'xiaotian',\n",
    "    'xiaowei',\n",
    "    'xiaoxiong',\n",
    "    'zhangyue1',\n",
    "    'zhuzhu',\n",
    "    'kk773510555',\n",
    "    'kuijhu',\n",
    "    'lokiu',\n",
    "    'oikjuy',\n",
    "    'qianhai18',\n",
    "    'saiwaiba',\n",
    "])\n",
    "\n",
    "\n",
    "DF_registration_train = DF_whole.query(\n",
    "    '(registration == 1) and (user_name not in @test_user_names)'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting pairs of keystroke dynamics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll train several binary classifiers which for a given pair of sequences will predict whether the pair was produced by the same user (1), or rather by two different users (0).\n",
    "\n",
    "For that, we will use the registration sequences, and pair them up to create a feature matrix to train the model.\n",
    "\n",
    "<img src=\"images/pairing.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "\n",
    " The following function, `extract_pairs`, produces a pandas DataFrame that contains the feature matrix, $\\mathbb{X}$, but also the labels, $\\mathbf{y}$ (column `'label'`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pair_data import extract_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important note\n",
    "We will train two types of models.\n",
    "\n",
    "The **first type of models** will receive as input a feature matrix, $\\mathbb{X}$, whose rows will be absolute differences of the feature vectors of the paired sequences:\n",
    "\n",
    "<img src=\"images/feature_difference.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "\n",
    "The **second type of models**, the siamese models, will receive a cross product of the features of the sequences, therefore the frame will have twice the width:\n",
    "\n",
    "<img src=\"images/feature_concat.png\" alt=\"Drawing\" style=\"width: 1000px;\"/>\n",
    "\n",
    "(Note that the above image shows a concatenation of two frames, rather than a cross product, but it correctly shows how the resultant frame might look like.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "## As an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "DF_train_all_pairs = extract_pairs(DF_registration_train, base_feature_names, only_feature_diffs=True)\n",
    "print(\n",
    "    'Fraction of positive observations: {:.4f}'\n",
    "    .format(DF_train_all_pairs['label'].mean())\n",
    ")\n",
    "DF_train_all_pairs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom cross validation score (AUC, actually) <a name=\"crossval\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "def custom_cross_val_auc_score(estimator, DF_registration,\n",
    "                               num_folds=4, seed=0):\n",
    "    \"\"\"Function for performing cross-validation of a model identifying\n",
    "    whether two keystroke dynamics come from the same user.\n",
    "    \n",
    "    We need to split the DF_registration frame with respect to \"user_name\"s\n",
    "    (to avoid data leakage), and then create the training and validation frames\n",
    "    (DF_train and DF_valid), which contain information about pairs of\n",
    "    keystroke dynamics (using the `extract_pairs` function).\n",
    "    \n",
    "    Args:\n",
    "        estimator: an object supplying the estimator.fit, and estimator.predict_proba\n",
    "            methods (preferably: an object of the class sklearn.pipeline.Pipeline\n",
    "            class).\n",
    "    \n",
    "        DF_registration: a pd.DataFrame that should contain only registration\n",
    "            data.\n",
    "            \n",
    "        num_folds: the number of folds for the cross validation split (the \"k\"\n",
    "            in \"k-fold cross-validation\").\n",
    "            \n",
    "        seed: a random numbe generator seed (passed to np.random.seed) to ensure\n",
    "            reproducibility of the results.\n",
    "        \n",
    "    Returns:\n",
    "        (auc_scores, best_classifier): a list of AUC scores, and the classifier\n",
    "            that produced the best AUC.\n",
    "    \"\"\"\n",
    "    user_names = DF_registration['user_name'].unique()\n",
    "    only_feature_diffs = estimator.only_feature_diffs\n",
    "    feature_names = estimator.feature_names\n",
    "    cv = KFold(num_folds)\n",
    "    np.random.seed(seed)\n",
    "    cv_iter = list(cv.split(user_names))\n",
    "    auc_scores = []\n",
    "    best_auc_score = -100000\n",
    "    best_classifier = None\n",
    "    best_y_true = None\n",
    "    best_y_pred = None\n",
    "    for train_user_names_selector, valid_user_names_selector in cv_iter:\n",
    "        if len(valid_user_names_selector) == 1:\n",
    "            print('Ignoring a fold with a single validation user'\n",
    "                  ' (consider lowering the value of `num_folds`)')\n",
    "            continue\n",
    "        estimator_copy = deepcopy(estimator)\n",
    "        train_user_names = user_names[train_user_names_selector]\n",
    "        DF_registration_train = DF_registration.query('user_name in @train_user_names')\n",
    "        DF_registration_valid = DF_registration.query('user_name not in @train_user_names')\n",
    "        DF_train = extract_pairs(DF_registration_train, feature_names, only_feature_diffs)\n",
    "        DF_valid = extract_pairs(DF_registration_valid, feature_names, only_feature_diffs)\n",
    "        y = DF_train['label']\n",
    "        X_train = DF_train.drop('label', 1)\n",
    "        \n",
    "        estimator_copy.fit(X_train, y)\n",
    "        \n",
    "        X_valid = DF_valid.drop('label', 1)\n",
    "        y_pred = estimator_copy.predict_proba(X_valid)\n",
    "        if y_pred.shape[1] == 2:\n",
    "            y_pred = y_pred[:, 1]\n",
    "        y_true = DF_valid['label']\n",
    "        auc_score = roc_auc_score(y_true, y_pred)\n",
    "        if auc_score > best_auc_score:\n",
    "            best_auc_score = auc_score\n",
    "            best_estimator = estimator_copy\n",
    "            best_y_true = y_true\n",
    "            best_y_pred = y_pred\n",
    "        auc_scores.append(auc_score)\n",
    "\n",
    "    return auc_scores, best_estimator, best_y_true, best_y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost -- our baseline estimator <a name=\"xgboost\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier, plot_importance\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "xgb_pipe = Pipeline([\n",
    "    ('XGBClassifier', XGBClassifier()),\n",
    "])\n",
    "\n",
    "# We're setting the following attributes manually;\n",
    "# they will be needed in the `custom_cross_val_auc_score`, and \n",
    "# `test_user_verification` functions, that's why I prefer\n",
    "# to have them as attributes of a Pipeline\n",
    "xgb_pipe.only_feature_diffs = True\n",
    "xgb_pipe.feature_names = base_feature_names\n",
    "\n",
    "\n",
    "scores, best_xgb_pipe, y_true, y_pred = custom_cross_val_auc_score(\n",
    "    xgb_pipe,\n",
    "    DF_registration_train,\n",
    ")\n",
    "\n",
    "plt.hist(scores, range=(0, 1), bins=20);\n",
    "xgb_classifier = best_xgb_pipe.steps[-1][1]\n",
    "plot_importance(xgb_classifier);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An interlude <a name=\"interlude\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC and the AUC score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Receiver Operating Characteristic (ROC) curve illustrates the performance of a binary classifier as its discrimination threshold (cutoff) is changed.\n",
    "\n",
    "The value of the Area Under the Curve (AUC) of the ROC is a score used to measure the quality of a classifier.\n",
    "\n",
    "The AUC measures how well model’s predictions rank the observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plots import plot_auc\n",
    "\n",
    "    \n",
    "plot_auc(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before talking about neural networks, let's see if our data need to be standardized..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(DF_train_all_pairs.drop('label', axis=1).values);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "# TODO(1): is there a better normalization method?\n",
    "\n",
    "\n",
    "norm_pipe = Pipeline([\n",
    "    ('RobustScaler', RobustScaler())\n",
    "])\n",
    "\n",
    "X_train_norm = norm_pipe.fit_transform(DF_train_all_pairs.drop('label', axis=1))\n",
    "\n",
    "plt.hist(X_train_norm);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But because we'll want to work with `pd.DataFrame` (we need column names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RobustScalerForPandas(RobustScaler):\n",
    "    \"\"\"A wrapper class, that performs fit/transform,\n",
    "    but keeps the data as a pd.DataFrame.\n",
    "    \n",
    "    Estimators defined further on -- MLPKerasWrapper, SimplestSiameseModel,\n",
    "    and RNNSiameseModel -- work only with pd.DataFrames, but the\n",
    "    sklearn's RobustScaler returns a numpy matrix.\n",
    "    Therefore, this wrapper is used in sklearn's Pipelines whenever we\n",
    "    use the aforementioned estimators.\n",
    "    \"\"\"\n",
    "    def fit(self, DF_data, y=None):\n",
    "        super().fit(DF_data.values)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, DF_data, y=None):\n",
    "        colnames = DF_data.columns\n",
    "        return pd.DataFrame(\n",
    "            super().transform(DF_data.values),\n",
    "            columns=colnames,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The multilayer perceptron (MLP) model <a name=\"mlp\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/mlp.png)\n",
    "\n",
    "Source: https://cs231n.github.io/neural-networks-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# TODO(3): ensure reproducibility with the tensorflow backend;\n",
    "# see Issue: https://github.com/keras-team/keras/issues/2280\n",
    "# (without sacrificing speed)\n",
    "import keras.backend as K\n",
    "from keras.layers import Input, Dense, Dropout, ELU\n",
    "from keras.models import Model\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.regularizers import l2\n",
    "\n",
    "\n",
    "class MLPKerasWrapper:\n",
    "    \"\"\"This is a wrapper class, allowing us to use a keras model in\n",
    "    sklearn's Pipeline.\n",
    "    \n",
    "    We want to be able to use instances of this class to perform cross-validation\n",
    "    in the custom_cross_val_auc_score function, and then to perform tests\n",
    "    with the test_user_verification function.\n",
    "    \n",
    "    Keras has its own wrappers (see the keras.wrappers.scikit_learn submodule),\n",
    "    but they were not sufficient for our purposes.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, inp_shape,\n",
    "                 num_epochs=10, num_neurons=40, num_layers=2, reg=1e-3):\n",
    "        self.inp_shape = inp_shape\n",
    "        self.num_neurons = num_neurons\n",
    "        self.num_layers = num_layers\n",
    "        self.reg = reg\n",
    "        self.num_epochs = num_epochs\n",
    "\n",
    "    def fit(self, DF_data, y):        \n",
    "        # Every time we're calling `fit`, we need a whole new set of initial\n",
    "        # weights in the model; I haven't found a clean way of doing this\n",
    "        # automatically in keras, so to avoid potential errors, we're\n",
    "        # re-initializing the model to make sure it's going to be fit\n",
    "        # from scratch\n",
    "        self._init_model()  # TODO(2): can we do better?\n",
    "        \n",
    "        # TODO(1): tweak the class_weight values\n",
    "        self.model.fit(\n",
    "            x=DF_data.values,\n",
    "            y=y,\n",
    "            verbose=2,\n",
    "            epochs=self.num_epochs,\n",
    "            class_weight=None,\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, DF_data):\n",
    "        return self.model.predict(DF_data.values)\n",
    "    \n",
    "    def _init_model(self):\n",
    "        inp = Input(shape=self.inp_shape)\n",
    "\n",
    "        x = Dense(self.num_neurons, kernel_regularizer=l2(self.reg))(inp)\n",
    "        x = ELU()(x)\n",
    "        for _ in range(self.num_layers-1):\n",
    "            x = Dense(self.num_neurons, kernel_regularizer=l2(self.reg))(x)\n",
    "            x = ELU()(x)\n",
    "\n",
    "        x = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "        self.model = Model(inputs=inp, outputs=x)\n",
    "        # TODO(1): try out different optimization algorithms\n",
    "        # and/or different learning rates\n",
    "        self.model.compile(\n",
    "            optimizer='adam',\n",
    "            loss='binary_crossentropy',\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mlp_pipe = Pipeline([\n",
    "    ('RobustScalerForPandas', RobustScalerForPandas()),\n",
    "    ('MLPKerasWrapper', MLPKerasWrapper(inp_shape=(len(base_feature_names), ))),\n",
    "])\n",
    "\n",
    "mlp_pipe.only_feature_diffs = True\n",
    "mlp_pipe.feature_names = base_feature_names\n",
    "\n",
    "\n",
    "scores, best_mlp_pipe, y_true, y_pred = custom_cross_val_auc_score(\n",
    "    mlp_pipe,\n",
    "    DF_registration_train,\n",
    ")\n",
    "\n",
    "plt.hist(scores, range=(0, 1), bins=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_auc(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Siamese models <a name=\"siamese\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_train_all_pairs = extract_pairs(DF_registration_train, base_feature_names, only_feature_diffs=False)\n",
    "print(\n",
    "    'Fraction of positive observations: {:.4f}'\n",
    "    .format(DF_train_all_pairs['label'].mean())\n",
    ")\n",
    "DF_train_all_pairs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Siamese neural network (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/siamese_mlp.png\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "code_folding": [],
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras.layers import Lambda\n",
    "\n",
    "\n",
    "class SimplestSiameseModel:\n",
    "    \"\"\"(Same intro as for MLPKerasWrapper.)\n",
    "    \n",
    "    This class implements a siamese architecture, in which the base model\n",
    "    is a simple MLP.\n",
    "    \"\"\"\n",
    "    def __init__(self, inp_shape,\n",
    "                 num_epochs=10, num_neurons=40, num_layers=2, reg=1e-3):\n",
    "        self.inp_shape = inp_shape\n",
    "        self.num_neurons = num_neurons\n",
    "        self.num_layers = num_layers\n",
    "        self.reg = reg\n",
    "        self.num_epochs = num_epochs\n",
    "\n",
    "    def fit(self, DF_data, y):\n",
    "        columns_A, columns_B = self._extract_columns(DF_data)\n",
    "        \n",
    "        self._init_model()\n",
    "        \n",
    "        self.siamese_net.fit(\n",
    "            x=[DF_data[columns_A].values, DF_data[columns_B].values],\n",
    "            y=y,\n",
    "            batch_size=32,\n",
    "            epochs=self.num_epochs,\n",
    "            verbose=2,\n",
    "        )\n",
    "\n",
    "    def predict_proba(self, DF_data):\n",
    "        columns_A, columns_B = self._extract_columns(DF_data)\n",
    "        return self.siamese_net.predict(\n",
    "            [DF_data[columns_A].values, DF_data[columns_B].values]\n",
    "        )\n",
    "    \n",
    "    def _init_model(self):\n",
    "        input_A = Input(self.inp_shape)\n",
    "        input_B = Input(self.inp_shape)\n",
    "\n",
    "        base_network = self._create_base_network()\n",
    "\n",
    "        leg_A = base_network(input_A)\n",
    "        leg_B = base_network(input_B)\n",
    "\n",
    "        # Merge two representations using an absolute difference between them\n",
    "        def abs_diff(x): return K.abs(x[0] - x[1])\n",
    "        diff = Lambda(function=abs_diff,\n",
    "                      output_shape=lambda x: x[0])([leg_A, leg_B])\n",
    "        x = Dense(self.num_neurons)(diff)\n",
    "        x = ELU()(x)\n",
    "        prediction = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "        self.siamese_net = Model([input_A, input_B], prediction)\n",
    "        self.siamese_net.compile(\n",
    "            optimizer='adam',\n",
    "            loss='binary_crossentropy',\n",
    "        )\n",
    "\n",
    "    def _create_base_network(self):\n",
    "        inp = Input(shape=self.inp_shape)\n",
    "        x = Dense(self.num_neurons, kernel_regularizer=l2(self.reg))(inp)\n",
    "        x = ELU()(x)\n",
    "        for _ in range(self.num_layers-1):\n",
    "            x = Dense(self.num_neurons, kernel_regularizer=l2(self.reg))(x)\n",
    "            x = ELU()(x)\n",
    "        return Model(inputs=inp, outputs=x)\n",
    "    \n",
    "    def _extract_columns(self, DF_data):\n",
    "        columns = DF_data.columns\n",
    "        columns_A = columns[columns.str.contains('_A')]\n",
    "        columns_B = columns[columns.str.contains('_B')]\n",
    "        return columns_A, columns_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "siamese_pipe = Pipeline([\n",
    "    ('RobustScalerForPandas', RobustScalerForPandas()),\n",
    "    ('SimplestSiameseModel', SimplestSiameseModel(inp_shape=(len(base_feature_names),))),\n",
    "])\n",
    "\n",
    "siamese_pipe.only_feature_diffs = False\n",
    "siamese_pipe.feature_names = base_feature_names\n",
    "\n",
    "\n",
    "scores, best_siamese_pipe, y_true, y_pred = custom_cross_val_auc_score(\n",
    "    siamese_pipe,\n",
    "    DF_registration_train,\n",
    ")\n",
    "\n",
    "plt.hist(scores, range=(0, 1), bins=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_auc(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Siamese neural network with recurrent cell(s) <a name=\"siamese_rnn\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/siamese_gru.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "\n",
    "The GRU (Gated Recurrent Unit) is an architecture whose state can hold more information than the state of a standard RNN unit (roughly speaking, it has a better \"memory\"), and can thus give better predictions.\n",
    "\n",
    "<img src=\"images/gru_colah.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "\n",
    "Source: http://colah.github.io/posts/2015-08-Understanding-LSTMs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import GRU, LSTM\n",
    "\n",
    "\n",
    "class RNNSiameseModel:\n",
    "    \"\"\"(Same doc as for MLPKerasWrapper).\n",
    "    \n",
    "    This class implements a siamese architecture, in which the base model\n",
    "    is a recurrent neural network (GRU, to be exact).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, inp_shape, max_length,\n",
    "                 num_epochs=10, num_neurons=40, num_layers=3, reg=1e-3):\n",
    "        self.inp_shape = inp_shape\n",
    "        self.num_neurons = num_neurons\n",
    "        self.num_layers = num_layers\n",
    "        self.reg = reg\n",
    "        self.num_epochs = num_epochs\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def fit(self, DF_data, y):\n",
    "        padded_seq_A = self._prep_sequences(DF_data['sequence_A'])\n",
    "        padded_seq_B = self._prep_sequences(DF_data['sequence_B'])\n",
    "        \n",
    "        self._init_model()\n",
    "        \n",
    "        self.siamese_net.fit(\n",
    "            x=[padded_seq_A, padded_seq_B],\n",
    "            y=y,\n",
    "            batch_size=32,\n",
    "            epochs=self.num_epochs,\n",
    "            verbose=2,\n",
    "        )\n",
    "\n",
    "    def predict_proba(self, DF_data):\n",
    "        padded_seq_A = self._prep_sequences(DF_data['sequence_A'])\n",
    "        padded_seq_B = self._prep_sequences(DF_data['sequence_B'])\n",
    "        return self.siamese_net.predict([padded_seq_A, padded_seq_B])\n",
    "    \n",
    "    def _init_model(self):\n",
    "        input_A = Input(shape=(None, self.max_length))\n",
    "        input_B = Input(shape=(None, self.max_length))\n",
    "\n",
    "        base_network = self._create_base_network()\n",
    "\n",
    "        leg_A = base_network(input_A)\n",
    "        leg_B = base_network(input_B)\n",
    "\n",
    "        # Merge two representations using the absolute difference between them\n",
    "        def abs_diff(x): return K.abs(x[0] - x[1])\n",
    "        diff = Lambda(function=abs_diff,\n",
    "                      output_shape=lambda x: x[0])([leg_A, leg_B])\n",
    "        x = Dense(self.num_neurons)(diff)\n",
    "        x = ELU()(x)\n",
    "        prediction = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "        self.siamese_net = Model([input_A, input_B], prediction)\n",
    "        self.siamese_net.compile(\n",
    "            optimizer='adam',\n",
    "            loss='binary_crossentropy',\n",
    "        )\n",
    "\n",
    "    def _create_base_network(self):\n",
    "        inp = Input(shape=(None, self.max_length))\n",
    "        x, state = GRU(128, return_state=True)(inp)  # TODO(1): can we do better?\n",
    "        x = ELU()(state)\n",
    "        return Model(inputs=inp, outputs=x)\n",
    "    \n",
    "    def _extract_colnames(self, columns):\n",
    "        columns_A = columns[columns.str.contains('_A')]\n",
    "        columns_B = columns[columns.str.contains('_B')]\n",
    "        return columns_A, columns_B\n",
    "    \n",
    "    def _prep_sequences(self, seq):\n",
    "        # TODO(2): this can be moved to a transformer\n",
    "        # that also normalizes the diffs\n",
    "        seq_diff = seq.apply(np.diff)\n",
    "        padded_seq = pad_sequences(seq_diff, maxlen=self.max_length)\n",
    "        return np.expand_dims(padded_seq, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_names = base_feature_names + ['sequence']\n",
    "max_length = DF_whole['sequence'].apply(len).max()\n",
    "\n",
    "# TODO(2): add a transformer (like RobustScalerForPandas)\n",
    "# that will preprocess the sequences, so that there's\n",
    "# no need of doing that in RNNSiameseModel._prep_sequences\n",
    "\n",
    "rnn_siamese_pipe = Pipeline([\n",
    "    ('RNNSiameseModel', \n",
    "     RNNSiameseModel(inp_shape=(len(base_feature_names), ), max_length=max_length)),\n",
    "])\n",
    "\n",
    "rnn_siamese_pipe.only_feature_diffs = False\n",
    "rnn_siamese_pipe.feature_names = feature_names\n",
    "\n",
    "\n",
    "# TODO(3): results for this model are considerably worse than\n",
    "# for others. Consider:\n",
    "# 1. applying the following sign_log function to the diffs:\n",
    "# def sign_log(x):\n",
    "#     return np.sign(x)*np.log1p(np.abs(x))\n",
    "# 2. adding dwell and flight times sequences\n",
    "scores, best_rnn_siamese_pipe, y_true, y_pred = custom_cross_val_auc_score(\n",
    "    rnn_siamese_pipe,\n",
    "    DF_registration_train,\n",
    ")\n",
    "\n",
    "plt.hist(scores, range=(0, 1), bins=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_auc(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests: user verification <a name=\"testing\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/testing_all_vs_one.png\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pair_data import get_pair_data\n",
    "\n",
    "\n",
    "def regist_vs_one_predictions(DF_regist, DF_one_row, pipe):\n",
    "    \"\"\"Function for making predictions for all pairs of rows\n",
    "    between the registration data (DF_regist), and the singular\n",
    "    row that needs to be verified (DF_one_row).\n",
    "    \n",
    "    Predictions are carried out by the pipe object, and the final\n",
    "    prediction is taken to be the mean of all predictions.\n",
    "    \n",
    "    TODO(3): can we do better? Can we utilize the information\n",
    "    about how similar/dissimilar the DF_one_row is to the rows\n",
    "    in DF_regist? Maybe a different statistic (max, min) works\n",
    "    better? \n",
    "    \"\"\"\n",
    "    DF_pair_data = get_pair_data(\n",
    "        pipe.only_feature_diffs,\n",
    "        DF_regist,\n",
    "        DF_one_row,\n",
    "    )\n",
    "    y_pred = pipe.predict_proba(DF_pair_data)\n",
    "    if y_pred.shape[1] == 2:\n",
    "        y_pred = y_pred[:, 1]\n",
    "        \n",
    "    return np.mean(y_pred)  # TODO(3): can we do better?\n",
    "\n",
    "\n",
    "def test_user_verification(pipe, test_user_names, DF_whole):\n",
    "    \"\"\"Function for testing the pairwise model, by checking whether\n",
    "    new keystroke dynamics match those provided during registration.\n",
    "    \n",
    "    Args:\n",
    "        pipe: Pipeline (or a bare estimator) for performing the matching;\n",
    "            it MUST have attributes only_feature_diffs, and feature_names,\n",
    "            which are required to produce a frame with the right columns\n",
    "            for pipe.\n",
    "            \n",
    "        test_user_names: a list of user names, which will be used for\n",
    "            testing.\n",
    "            \n",
    "        DF_whole: a pd.DataFrame with both registration, and test rows.\n",
    "            \n",
    "    Returns:\n",
    "        A list of AUC scores calculated from the verification.\n",
    "    \"\"\"\n",
    "    feature_names = pipe.feature_names\n",
    "    only_feature_diffs = pipe.only_feature_diffs\n",
    "    \n",
    "    auc_scores = []\n",
    "    for user_name in test_user_names:\n",
    "        DF_one_user = DF_whole.query('user_name == @user_name')\n",
    "        DF_regist = DF_one_user.query('registration == 1')[feature_names]\n",
    "        DF_not_regist = DF_one_user.query('registration != 1')\n",
    "        \n",
    "        same_person = 1 - DF_not_regist['imposter']\n",
    "        if same_person.sum() == 0:\n",
    "            print('No positive examples for user \"{}\"'.format(user_name))\n",
    "            continue\n",
    "        if same_person.sum() == len(same_person):\n",
    "            print('No negative examples for user \"{}\"'.format(user_name))\n",
    "            continue\n",
    "\n",
    "        num_rows = DF_not_regist.shape[0]\n",
    "        preds = []\n",
    "        for row_number in range(num_rows):\n",
    "            DF_one_row = DF_not_regist[feature_names].iloc[[row_number]]\n",
    "            pred = regist_vs_one_predictions(DF_regist, DF_one_row, pipe)\n",
    "            preds.append(pred)\n",
    "\n",
    "        auc_score = roc_auc_score(same_person, preds)\n",
    "        auc_scores.append(auc_score)\n",
    "        \n",
    "        msg = 'AUC score: {:.2f} for user \"{}\"'.format(auc_score, user_name)\n",
    "        if auc_score <= 0.5:\n",
    "            msg += ' (definitely wrong)'\n",
    "        print(msg)\n",
    "        \n",
    "    print('*** Mean AUC score: {:.4f} ***'.format(np.mean(auc_scores)))\n",
    "    return auc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('XGB pipeline')\n",
    "scores = test_user_verification(best_xgb_pipe, test_user_names, DF_whole)\n",
    "plt.hist(scores, range=(0, 1), bins=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('MLP pipeline')\n",
    "scores = test_user_verification(best_mlp_pipe, test_user_names, DF_whole)\n",
    "plt.hist(scores, range=(0, 1), bins=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "print('Simple siamese pipeline')\n",
    "scores = test_user_verification(best_siamese_pipe, test_user_names, DF_whole)\n",
    "plt.hist(scores, range=(0, 1), bins=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RNN siamese pipeline')\n",
    "scores = test_user_verification(best_rnn_siamese_pipe, test_user_names, DF_whole);\n",
    "plt.hist(scores, range=(0, 1), bins=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
